{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF3bVPPUhALv7IMtx8IRt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sashwot10/5CS037/blob/main/worksheet8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BjhavgmxeMNH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
        "\n",
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        \"\"\"Initializes the tree with a specified maximum depth[cite: 50, 163].\"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"Calculates entropy using H = -sum(p * log2(p))[cite: 208, 217].\"\"\"\n",
        "        if len(y) == 0: return 0\n",
        "        class_probs = np.bincount(y) / len(y)\n",
        "        # Add epsilon (1e-9) to avoid log(0) [cite: 139, 218]\n",
        "        return -np.sum([p * np.log2(p + 1e-9) for p in class_probs if p > 0])\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        \"\"\"Computes reduction in entropy after a split[cite: 107, 197].\"\"\"\n",
        "        parent_entropy = self._entropy(parent)\n",
        "        n = len(parent)\n",
        "        weighted_child_entropy = (len(left) / n) * self._entropy(left) + (len(right) / n) * self._entropy(right)\n",
        "        return parent_entropy - weighted_child_entropy\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively builds the tree by finding the best split[cite: 63, 177].\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Stopping conditions: pure node or max depth reached [cite: 74, 191]\n",
        "        if len(unique_classes) == 1 or (self.max_depth and depth >= self.max_depth) or num_samples <= 1:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        best_gain = -1\n",
        "        best_split = None\n",
        "\n",
        "        for feature_idx in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature_idx] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                if np.any(left_mask) and np.any(right_mask):\n",
        "                    gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_idx': feature_idx,\n",
        "                            'threshold': threshold,\n",
        "                            'left_mask': left_mask,\n",
        "                            'right_mask': right_mask\n",
        "                        }\n",
        "\n",
        "        if best_split is None:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        # Recursive step [cite: 101, 195]\n",
        "        left_tree = self._build_tree(X[best_split['left_mask']], y[best_split['left_mask']], depth + 1)\n",
        "        right_tree = self._build_tree(X[best_split['right_mask']], y[best_split['right_mask']], depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_tree': left_tree,\n",
        "            'right_tree': right_tree\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Trains the model[cite: 55, 171].\"\"\"\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _predict_single(self, x, tree):\n",
        "        \"\"\"Traverses the tree for a single sample[cite: 146, 226].\"\"\"\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "        if x[tree['feature_idx']] <= tree['threshold']:\n",
        "            return self._predict_single(x, tree['left_tree'])\n",
        "        else:\n",
        "            return self._predict_single(x, tree['right_tree'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts labels for a matrix X[cite: 137, 220].\"\"\"\n",
        "        return [self._predict_single(x, self.tree) for x in X]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Split data [cite: 236]\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Tree [cite: 252]\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "acc_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Scikit-Learn Tree [cite: 263]\n",
        "sk_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sk_tree.fit(X_train, y_train)\n",
        "y_pred_sk = sk_tree.predict(X_test)\n",
        "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
        "\n",
        "print(f\"Custom Tree Accuracy: {acc_custom:.4f}\")\n",
        "print(f\"Scikit-learn Accuracy: {acc_sk:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPUepIoTfx02",
        "outputId": "9f61e03a-e4c3-4961-ddcf-fe7ace5c4932"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tree Accuracy: 1.0000\n",
            "Scikit-learn Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Wine Dataset [cite: 281]\n",
        "wine = load_wine()\n",
        "X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Classifier & Tuning [cite: 283, 286]\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='f1_macro')\n",
        "grid_search.fit(X_w_train, y_w_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_w_test)\n",
        "print(f\"Best RF F1 Score: {f1_score(y_w_test, y_pred_rf, average='macro'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNoZ1LJsf4I_",
        "outputId": "a7fcb652-d5f7-473b-be60-5596f7b3fb9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RF F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor & Tuning [cite: 290, 291]\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(rf_reg, param_dist, n_iter=5, cv=5, random_state=42)\n",
        "random_search.fit(X_w_train, y_w_train)\n",
        "\n",
        "print(f\"Best Regression Parameters: {random_search.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7Iwsc6kf_UF",
        "outputId": "bb67eeea-f7a6-4d3f-af30-7d2ff54674b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Regression Parameters: {'n_estimators': 100, 'min_samples_leaf': 1, 'max_features': 'log2'}\n"
          ]
        }
      ]
    }
  ]
}